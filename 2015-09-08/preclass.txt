Pre-Class Questions:

Consider the following naive row-based N x N matmul (matrix multiplication):

for (i = 0; i < N; i++){
   for (j = 0; j < N; j++){
      tmp = 0
      for (k = 0; k < N; k++)
         tmp += A[i,k] * B[k,j]
   }
      C[i,j] = tmp
}

Suppose data is in double-precsion floating point. We are interested in
estimating the memory-based arithmetic intensity (AI) of this code. The
memory-based AI is defined that (# flops) / (# bytes transferred between memory
and cache), and depends on the cache size. Suppose the cache uses a
least-recently-used (LRU) policy for deciding which data to flush when moving
something into an already-full cache.

1. Suppose 16N is significantly larger than the size of our L3 cache. What is
the memory-based AI of this code? (Hint: What is the memory-based AI of just the
innermost loop?)

Doesn't it depend on the "# flops" in the numerator? What is the speed of the 
processor we're running on? Is it assumed to be the Xeon processors in the cluster?
Either way, I would expect the denominator to be large, because if one complete  
run of the inner loop (N iterations loading 16 bytes each) overflows the cache, 
each iteration of the middle loop will need to transfer all 16N bytes from memory
to cache.

2. Now suppose that the cache is substantially larger than 16N, but
substantially smaller than 8N^2. What is the AI now?

Well, I still don't know how to calculate the numerator, but the denominator will 
now be smaller, because the 16N bytes loaded by one complete run of the inner 
loop will stay in cache and be available to the next iteration of the middle 
loop. Since the bytes loaded from A row i will still be useful on the next 
iteration (in which only j has been incremented), the next iteration of the 
middle loop will only load the 8N bytes from B column j+1 into memory.

3. Now suppose the cache is large enough to hold all of A, B, and C. What is the
AI now? (Hint: Writing to a byte of memory not already in the cache incurs two
memory transfers: one to move the data to the cache for writing, and one to move
the written data back to main memory.)

I don't have time to figure out how this temporary matrix "C" works. I've never 
seen this "naive matrix multiplication" before and I don't understand how it works.


4. Cache overflowing. On my CPU (Intel i7-4700 HQ), L1, L2, and L3 caches are 32
KB, 256 KB, and 6 MB respectively. What is the largest problem size N that will
fit in each cache? What is the arithmetic intensity associated with each problem
size?


5. My CPU has 4 cores, each of which can do 8 fused multiply-adds per cycle, has
a clock rate of 2.4 GHz, and a memory bandwidth of 25.6 GB/s. At what arithmetic
intensity does my machine become CPU-bound?


6. So, for what size range for N will naive matmul be CPU-bound on my machine?


7. So, what will a plot of Flops/sec vs N look like?
