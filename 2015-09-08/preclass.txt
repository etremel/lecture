Pre-Class Questions:

Consider the following naive row-based N x N matmul (matrix multiplication):

for (i = 0; i < N; i++){
   for (j = 0; j < N; j++){
      tmp = 0
      for (k = 0; k < N; k++)
         tmp += A[i,k] * B[k,j]
   }
      C[i,j] = tmp
}

Suppose data is in double-precsion floating point. We are interested in
estimating the memory-based arithmetic intensity (AI) of this code. The
memory-based AI is defined that (# flops) / (# bytes transferred between memory
and cache), and depends on the cache size. Suppose the cache uses a
least-recently-used (LRU) policy for deciding which data to flush when moving
something into an already-full cache.

1. Suppose 16N is significantly larger than the size of our L3 cache. What is
the memory-based AI of this code? (Hint: What is the memory-based AI of just the
innermost loop?)

Unless there are FMA shenanigans going on, each iteration of the inner loop takes
2 flops to compute (a multiply and an add). So it takes 2N^3 flops to compute every
element of matrix C, because each cell (C[i,j] = tmp) takes N iterations of the inner 
loop to compute, and there are N^2 cells. For memory accesses, if one complete  
run of the inner loop (N iterations loading 16 bytes each) overflows the cache, 
each iteration of the middle loop will need to transfer all 16N bytes from memory
to cache. The middle loop runs N times, for 16N^2 total bytes transferred, and
the outer loop runs N times, making the total 16N^3 bytes by the time the outer
loop is finished. So AI = 2N^3 / 16N^3 = 1/8 for this program.

2. Now suppose that the cache is substantially larger than 16N, but
substantially smaller than 8N^2. What is the AI now?

The number of flops hadn't changed, but the number of memory accesses will now 
be smaller, because the 16N bytes loaded by one complete run of the inner 
loop will stay in cache and be available to the next iteration of the middle 
loop. Since the bytes loaded from A row i will still be useful on the next 
iteration (in which only j has been incremented), the next iteration of the 
middle loop will only load the 8N bytes from B column j+1 into memory. On the 
other hand, since 8N^2 doesn't fit into cache, a complete run of the middle loop
will overflow the cache, so there is no re-use between iterations of the outer loop.
This means a complete run of the middle loop will transfer 8N^2 total bytes, and
all N iterations of the outer loop will transfer 8N^3 total bytes. So now 
AI = 2N^3 / 8N^3 = 1/4.

3. Now suppose the cache is large enough to hold all of A, B, and C. What is the
AI now? (Hint: Writing to a byte of memory not already in the cache incurs two
memory transfers: one to move the data to the cache for writing, and one to move
the written data back to main memory.)

Even though all the matrices fit in memory, the data still needs to be loaded the
first time it is accessed, so you need to load at least 8N^2 + 8N^2 = 16N^2 bytes to
load all of matrices A and B (each is N^2 cells of 8 bytes each). Given the hint, 
writing all N cells of C takes 16N^2 bytes of memory access, so there's a total of
32N^2 bytes of memory access to read and write everything in cache. With the same
number of flops, AI = 2N^3 / 32N^2 = N/16.

4. Cache overflowing. On my CPU (Intel i7-4700 HQ), L1, L2, and L3 caches are 32
KB, 256 KB, and 6 MB respectively. What is the largest problem size N that will
fit in each cache? What is the arithmetic intensity associated with each problem
size?


5. My CPU has 4 cores, each of which can do 8 fused multiply-adds per cycle, has
a clock rate of 2.4 GHz, and a memory bandwidth of 25.6 GB/s. At what arithmetic
intensity does my machine become CPU-bound?


6. So, for what size range for N will naive matmul be CPU-bound on my machine?


7. So, what will a plot of Flops/sec vs N look like?
