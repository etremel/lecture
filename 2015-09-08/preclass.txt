Pre-Class Questions:

Consider the following naive row-based N x N matmul (matrix multiplication):

for (i = 0; i < N; i++){
   for (j = 0; j < N; j++){
      tmp = 0
      for (k = 0; k < N; k++)
         tmp += A[i,k] * B[k,j]
   }
      C[i,j] = tmp
}

Suppose data is in double-precision floating point. We are interested in
estimating the memory-based arithmetic intensity (AI) of this code. The
memory-based AI is defined that (# flops) / (# bytes transferred between memory
and cache), and depends on the cache size. Suppose the cache uses a
least-recently-used (LRU) policy for deciding which data to flush when moving
something into an already-full cache.

1. Suppose 16N is significantly larger than the size of our L3 cache. What is
the memory-based AI of this code? (Hint: What is the memory-based AI of just the
innermost loop?)

Unless there are FMA shenanigans going on, each iteration of the inner loop takes
2 flops to compute (a multiply and an add). So it takes 2N^3 flops to compute every
element of matrix C, because each cell (C[i,j] = tmp) takes N iterations of the inner 
loop to compute, and there are N^2 cells. For memory accesses, if one complete  
run of the inner loop (N iterations loading 16 bytes each) overflows the cache, 
each iteration of the middle loop will need to transfer all 16N bytes from memory
to cache. The middle loop runs N times, for 16N^2 total bytes transferred, and
the outer loop runs N times, making the total 16N^3 bytes by the time the outer
loop is finished. So AI = 2N^3 / 16N^3 = 1/8 for this program.

2. Now suppose that the cache is substantially larger than 16N, but
substantially smaller than 8N^2. What is the AI now?

The number of flops hadn't changed, but the number of memory accesses will now 
be smaller, because the 16N bytes loaded by one complete run of the inner 
loop will stay in cache and be available to the next iteration of the middle 
loop. Since the bytes loaded from A row i will still be useful on the next 
iteration (in which only j has been incremented), the next iteration of the 
middle loop will only load the 8N bytes from B column j+1 into memory. On the 
other hand, since 8N^2 doesn't fit into cache, a complete run of the middle loop
will overflow the cache, so there is no re-use between iterations of the outer loop.
This means a complete run of the middle loop will transfer 8N^2 total bytes, and
all N iterations of the outer loop will transfer 8N^3 total bytes. So now 
AI = 2N^3 / 8N^3 = 1/4.

3. Now suppose the cache is large enough to hold all of A, B, and C. What is the
AI now? (Hint: Writing to a byte of memory not already in the cache incurs two
memory transfers: one to move the data to the cache for writing, and one to move
the written data back to main memory.)

Even though all the matrices fit in memory, the data still needs to be loaded the
first time it is accessed, so you need to load at least 8N^2 + 8N^2 = 16N^2 bytes to
load all of matrices A and B (each is N^2 cells of 8 bytes each). Given the hint, 
writing all N cells of C takes 16N^2 bytes of memory access, so there's a total of
32N^2 bytes of memory access to read and write everything in cache. With the same
number of flops, AI = 2N^3 / 32N^2 = N/16.

4. Cache overflowing. On my CPU (Intel i7-4700 HQ), L1, L2, and L3 caches are 32
KB, 256 KB, and 6 MB respectively. What is the largest problem size N that will
fit in each cache? What is the arithmetic intensity associated with each problem
size?

Let's assume that in order for the problem to "fit in" cache, all 3 matrices need
to fit. Technically matrix C doesn't need to fit because each element of C is only
written once, but if the cache isn't big enough to hold all 3 matrices, writing
an element of C might evict a cached element of B that would be needed in the 
next iteration of the outer loop (since the CPU will automatically cache the 
element of C that was written). How much extra space is needed to make sure that
A and B stay cached despite C being written depends on the associativity of the 
cache, and I don't know how to estimate that. Under this assumption, the cache 
needs to be at least 24N^2 bytes (to hold 8N^2 bytes per matrix), so to fit in
the 32KB L1 cache, N can be at most sqrt(32000/24) = 36 (rounded down). For the 
problem to fit in the L2 cache, N can be at most 103, and for the problem to fit
in the L3 cache, N can be at most 500.

To calculate the arithmetic intensity, I'll assume that the formula from problem
3 applies, since all the matrices will fit in (some level of) cache. For N=36, 
AI = 36/16 = 2.25. For N=103, AI = 103/16 = 6.44. For N=500, AI = 500/16 = 31.25.

5. My CPU has 4 cores, each of which can do 8 fused multiply-adds per cycle, has
a clock rate of 2.4 GHz, and a memory bandwidth of 25.6 GB/s. At what arithmetic
intensity does my machine become CPU-bound?

The peak flop rate is 8 FMA/cycle/core * 4 cores * 2400000000 cycles/second = 
76.8 Gflop/second. In order for the computation to be CPU-bound, the CPU must 
do at least 76.8 billion flops for every 25.6 billion bytes of data (assuming 
the data it gets in 1 second is used for computation that same second), so the 
code needs to have an AI of 76.8/25.6 = 3.

6. So, for what size range for N will naive matmul be CPU-bound on my machine?

Since the small sizes of N fit entirely in cache and have N / 16 arithmetic 
intensity, the smallest N that will have AI at least 3 is 48. For N > 500 the
data will not fit in cache and the arithmetic intensity becomes a constant 1/4.
So the range of N that will be CPU-bound is 48-500.

7. So, what will a plot of Flops/sec vs N look like?

It will rise from N=1 to N=48, then hit a plateau at the CPU flops/sec limit for
N=48 to N=500, then decrease after N=500 as there are more and more cache misses.
